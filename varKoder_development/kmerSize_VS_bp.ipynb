{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test effect of kmer size and base pairs used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will test the effect of kmer size used to produce images and of amount of data in each image in validation accuracy after training.\n",
    "\n",
    "In all cases, we will use Label Smoothing, Cut Mix, and a pretrained xresnet50. We will include all samples sequenced for a given amount of data, regardless of the sequencing quality. Three randomly chosen samples will be used for validation and the remainder for training. Training will be done in 15 epochs, and the batch size will be chosen so that that are approximately 10 batches per epoch (up to a maximum size of 64). For each combination of kmer-size and amount of data, we will do 5 replicates with a different training/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-32GB'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from fastai.data.all import DataBlock, ColSplitter, ColReader\n",
    "from fastai.vision.all import ImageBlock, CategoryBlock, cnn_learner, xresnet50\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.learner import load_learner\n",
    "from fastai.torch_core import set_seed\n",
    "from torch.cuda import get_device_name\n",
    "from fastai.callback.mixup import CutMix\n",
    "from fastai.losses import LabelSmoothingCrossEntropyFlat\n",
    "from math import log\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: select training and validation set given kmer size and amount of data (as a list).\n",
    "We will only consider samples that have yielded at least 200M bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set(kmer_size, bp_training):\n",
    "    file_path = [x.absolute() for x in (Path('images_' + str(kmer_size))).ls() if x.suffix == '.png']\n",
    "    taxon = [x.name.split('__')[0] for x in file_path]\n",
    "    sample = [x.name.split('__')[-1].split('_')[0] for x in file_path]\n",
    "    n_bp = [int(x.name.split('_')[-1].split('.')[0].replace('K','000')) for x in file_path]\n",
    "\n",
    "    df = pd.DataFrame({'taxon': taxon,\n",
    "                  'sample': sample,\n",
    "                  'n_bp': n_bp,\n",
    "                  'path': file_path\n",
    "                 })\n",
    "    \n",
    "    included_samples = df.loc[df['n_bp'] == 200000000]['sample']\n",
    "    df = df.loc[df['sample'].isin(included_samples)]\n",
    "\n",
    "    valid = (df[['taxon','sample']].\n",
    "             drop_duplicates().\n",
    "             groupby('taxon').\n",
    "             apply(lambda x: x.sample(3, replace=False)).\n",
    "             reset_index(drop=True).\n",
    "             assign(is_valid = True).\n",
    "             merge(df, on = ['taxon','sample'])\n",
    "        )\n",
    "\n",
    "    train = (df.loc[~df['sample'].isin(valid['sample'])].\n",
    "             assign(is_valid = False)\n",
    "            )\n",
    "    train = train.loc[train['n_bp'].isin(bp_training)]\n",
    "\n",
    "    train_valid = pd.concat([valid, train]).reset_index(drop = True)\n",
    "    return train_valid                          \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: create a learner and fit model, setting batch size according to number of training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(df, model = xresnet50, epochs = 30, normalize = True, pretrained = True, callbacks = CutMix, loss_fn = LabelSmoothingCrossEntropyFlat()):\n",
    "    #find a batch size that is a power of 2 and splits the dataset in about 10 batches\n",
    "    batch_size = min(2**round(log(df[~df['is_valid']].shape[0]/10,2)), 64) \n",
    "    \n",
    "    #start data block\n",
    "    dbl = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter = ColSplitter(),\n",
    "                       get_x = ColReader('path'),\n",
    "                       get_y = ColReader('taxon'),\n",
    "                       item_tfms = None,\n",
    "                       batch_tfms = None\n",
    "                      )\n",
    "    \n",
    "    #create data loaders with calculated batch size\n",
    "    dls = dbl.dataloaders(df, bs = batch_size)\n",
    "    \n",
    "    #create learner and train for 15 epochs\n",
    "    learn = cnn_learner(dls, \n",
    "                    model, \n",
    "                    metrics = accuracy, \n",
    "                    normalize = normalize,\n",
    "                    pretrained = pretrained,\n",
    "                    cbs = callbacks,\n",
    "                    loss_func = loss_fn\n",
    "                   ).to_fp16()\n",
    "    \n",
    "    #to speed up training, we will skip validation\n",
    "    valid = learn.dls.valid\n",
    "    learn.dls.valid = []\n",
    "    \n",
    "    with learn.no_logging():\n",
    "        learn.fine_tune(epochs = epochs, freeze_epochs = 0, base_lr = 1e-3)\n",
    "        \n",
    "    #now we can put the validation dataloader back    \n",
    "    learn.dls.valid = valid\n",
    "    \n",
    "    return(learn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function: get overall accuracy given a model, a table and amount of basepairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(learner, df, bp_test):\n",
    "    test = df[(df['is_valid'] == True) & df['n_bp'].isin(bp_test)]\n",
    "    tdl = learner.dls.test_dl(test, with_labels = True)\n",
    "    return learner.validate(dl = tdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined functions, let's test do a for loop to do the test for all combinations of kmer length, bp_training, bp_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using 200000000 bp for kmer size 9\n",
      "Replicate 9\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording accuracy\n",
      "500000 0.20000000298023224\n",
      "1000000 0.36666667461395264\n",
      "2000000 0.46666666865348816\n",
      "5000000 0.800000011920929\n",
      "10000000 0.8333333134651184\n",
      "20000000 0.8666666746139526\n",
      "50000000 0.8999999761581421\n",
      "100000000 0.8666666746139526\n",
      "200000000 0.8666666746139526\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "set_seed(1773541)\n",
    "\n",
    "with open('kmerSize_VS_bp.txt','w') as outfile:\n",
    "    for replicate in range(10):        \n",
    "        for kmer_len in range(5,10):\n",
    "            clear_output()\n",
    "            print('Training using all bp amounts for kmer size', kmer_len)\n",
    "            print('Replicate', replicate)\n",
    "            all_bp_tr = sorted([mult * 10**power for mult in [1,2,5] for power in range(5,9) \n",
    "                                if mult * 10**power <= 200000000 and mult * 10**power >= 500000])\n",
    "\n",
    "            #training with all bp amounts\n",
    "            df = get_training_set(kmer_len, all_bp_tr)\n",
    "            learn = train_cnn(df)\n",
    "\n",
    "            #recording validation accuracy for different bp amounts\n",
    "            print('Recording accuracy')\n",
    "            for bp_valid in all_bp_tr:\n",
    "                with learn.no_bar():\n",
    "                    acc = get_accuracy(learn, df, [bp_valid])\n",
    "                print(bp_valid, acc[1])\n",
    "                \n",
    "                entry = dict(kmer_size=kmer_len,\n",
    "                             replicate = replicate,\n",
    "                             bp_training = '|'.join([str(x) for x in sorted(all_bp_tr)]),\n",
    "                             bp_valid = bp_valid,\n",
    "                             samples_training = '|'.join(df.loc[~df['is_valid']]['sample'].drop_duplicates().sort_values().tolist()),\n",
    "                             n_samp_training = df.loc[~df['is_valid']]['sample'].drop_duplicates().shape[0],\n",
    "                             samples_valid = '|'.join(df.loc[df['is_valid']]['sample'].drop_duplicates().sort_values().tolist()),\n",
    "                             n_samp_valid = df.loc[df['is_valid']]['sample'].drop_duplicates().shape[0],\n",
    "                             valid_loss = acc[0],\n",
    "                             valid_acc = acc[1])\n",
    "                print(entry, file = outfile)\n",
    "                \n",
    "            \n",
    "            #training with specific bp amounts\n",
    "            for bp_train in all_bp_tr:\n",
    "                clear_output()\n",
    "                print('Training using', str(bp_train),'bp for kmer size', kmer_len)\n",
    "                print('Replicate', replicate)\n",
    "                \n",
    "                df = get_training_set(kmer_len, [bp_train])\n",
    "                learn = train_cnn(df)\n",
    "                print('Recording accuracy')\n",
    "                for bp_valid in all_bp_tr:\n",
    "                    with learn.no_bar():\n",
    "                        acc = get_accuracy(learn, df, [bp_valid])\n",
    "                    print(bp_valid, acc[1])\n",
    "                    entry = dict(kmer_size=kmer_len,\n",
    "                                 replicate = replicate,\n",
    "                                 bp_training = bp_train,\n",
    "                                 bp_valid = bp_valid,\n",
    "                                 samples_training = '|'.join(df.loc[~df['is_valid']]['sample'].drop_duplicates().sort_values().tolist()),\n",
    "                                 n_samp_training = df.loc[~df['is_valid']]['sample'].drop_duplicates().shape[0],\n",
    "                                 samples_valid = '|'.join(df.loc[df['is_valid']]['sample'].drop_duplicates().sort_values().tolist()),\n",
    "                                 n_samp_valid = df.loc[df['is_valid']]['sample'].drop_duplicates().shape[0],\n",
    "                                 valid_loss = acc[0],\n",
    "                                 valid_acc = acc[1])\n",
    "                    print(entry, file = outfile)\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we finished training and testing all models, let's save results as a table that can be easily read in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kmerSize_VS_bp.txt','r') as infile:\n",
    "    df = pd.DataFrame([eval(x) for x in infile])\n",
    "    df.to_csv('kmerSize_VS_bp.csv')\n",
    "\n",
    "Path('kmerSize_VS_bp.txt').unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-barcoding]",
   "language": "python",
   "name": "conda-env-.conda-barcoding-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
