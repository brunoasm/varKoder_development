---
title: "Creating tables from profiling results"
output: html_notebook
---

```{r}
library(tidyverse)
```


In this document, we will summarize profiling results in different computers and different tasks to create a supplement on computing resources.

## Bembidion tests

Let's start with a test using Bembidion data only.Let's create a function to read a log file and convert to a table.
```{r}
parse_profiling_log <- function(log_file_path) {
  # Helper function to convert time string to seconds
  convert_to_seconds <- function(time_str) {
    if (is.null(time_str) || is.na(time_str)) return(NA)
    
    parts <- str_split(time_str, ":")[[1]]
    if (length(parts) == 2) {
      # m:ss format
      mins <- as.numeric(parts[1])
      secs <- as.numeric(sub("\\..*$", "", parts[2]))
      return(mins * 60 + secs)
    } else if (length(parts) == 3) {
      # h:mm:ss format
      hours <- as.numeric(parts[1])
      mins <- as.numeric(parts[2])
      secs <- as.numeric(sub("\\..*$", "", parts[3]))
      return(hours * 3600 + mins * 60 + secs)
    }
    return(NA)
  }
  
  # Read the entire file
  log_content <- readLines(log_file_path)
  
  # Initialize lists to store results
  commands <- list()
  metrics <- list()
  
  current_command <- NULL
  current_metrics <- list()
  
  # Process each line
  for (line in log_content) {
    # Look for command being timed
    if (grepl("Command being timed:", line)) {
      if (!is.null(current_command)) {
        commands <- append(commands, list(current_command))
        metrics <- append(metrics, list(current_metrics))
      }
      current_command <- str_trim(sub("Command being timed: \"", "", sub("\"$", "", line)))
      current_metrics <- list()
      next
    }
    
    # Parse metric lines
    if (grepl("^[A-Z0-9]+: User time \\(seconds\\): ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["User_time_seconds"]] <- as.numeric(parts[3])
    } else if (grepl("^[A-Z0-9]+: System time \\(seconds\\): ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["System_time_seconds"]] <- as.numeric(parts[3])
    } else if (grepl("^[A-Z0-9]+: Percent of CPU this job got: ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["Percent_of_CPU"]] <- as.numeric(sub("%", "", parts[3]))
    } else if (grepl("^[A-Z0-9]+: Maximum resident set size \\(kbytes\\): ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["Maximum_resident_set_size_kbytes"]] <- as.numeric(parts[3])
    } else if (grepl("^[A-Z0-9]+: Elapsed \\(wall clock\\) time", line)) {
      time_value <- str_extract(line, "[0-9]+:[0-9]+(?:\\.[0-9]+)?$|[0-9]+:[0-9]+:[0-9]+(?:\\.[0-9]+)?$")
      current_metrics[["Elapsed_time_seconds"]] <- convert_to_seconds(time_value)
    }
  }
  
  # Add the last command if exists
  if (!is.null(current_command)) {
    commands <- append(commands, list(current_command))
    metrics <- append(metrics, list(current_metrics))
  }
  
  # Convert to tibble
  result_df <- tibble(
    code = str_extract(unlist(commands), "^\\S+"),
    User_time_seconds = sapply(metrics, `[[`, "User_time_seconds"),
    System_time_seconds = sapply(metrics, `[[`, "System_time_seconds"),
    Elapsed_time_seconds = sapply(metrics, `[[`, "Elapsed_time_seconds"),
    Percent_of_CPU = sapply(metrics, `[[`, "Percent_of_CPU"),
    maxRSS_GB = sapply(metrics, `[[`, "Maximum_resident_set_size_kbytes") / (1024 * 1024),
    command = str_trim(str_remove(unlist(commands), "^\\S+ "))
  )
  
  return(result_df)
}
```

Let's now apply this function.
```{r}
result_computers = list.files(pattern=".*tests.log") %>%
    set_names(., tools::file_path_sans_ext(.)) %>%
    purrr::map_df(parse_profiling_log, .id = "filename")

result_computers 

write_csv(result_computers, file = 'result_tables/different_computers.csv')

```

## CGR vs representation test

Let's start by reading the data table and standardizing column names to the table above. This table was generated by a python script parsing a log file in the analysis folder.
```{r}
df = read_csv("../varKoder_tests/additional_tests_cgr/computing_summary.csv") %>%
  mutate(maxRSS_GB=max_resident_kb/(1024*1024)) %>%
  select(phase, 
         architecture,
         representation,
         validation_sample,
         Elapsed_time_seconds=elapsed_time,
         User_time_seconds=user_time,
         System_time_seconds=system_time,
         Percent_of_CPU=cpu_percent,
         maxRSS_GB
         )

df
```

Let's now ask:

What is the average time taken for training 99 samples for each combination of architecture. We will separate both training phases
```{r}
training_summary = df %>%
  filter(phase!='query') %>%
  group_by(phase, architecture, representation, validation_sample) %>%
  group_by(phase, architecture, representation) %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),
            Mean_Percent_of_CPU=mean(Percent_of_CPU),
            mean_maxRSS_GB=mean(maxRSS_GB))

training_summary


training_summary %>% 
 split(training_summary$phase) %>% 
 purrr::walk2(names(.), ., ~write_csv(.y, file.path("result_tables", paste0("arch_vs_rep_", .x, ".csv"))))
```
What is the average time taken for for querying all varKodes for each sample?
```{r}
query_summary = df %>%
  filter(phase=='query') %>%
  group_by(architecture,representation,validation_sample) %>%
  summarise(User_time_seconds=sum(User_time_seconds),
            Elapsed_time_seconds=sum(Elapsed_time_seconds),
            System_time_seconds=sum(System_time_seconds),
            Percent_of_CPU=mean(Percent_of_CPU),
            maxRSS_GB=max(maxRSS_GB)) %>%
  group_by(architecture,representation) %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),
            Mean_Percent_of_CPU=mean(Percent_of_CPU),
            mean_maxRSS_GB=mean(maxRSS_GB))

query_summary
write_csv(query_summary, file.path("result_tables", paste0("arch_vs_rep_query.csv")))
```

## Conventional barcodes
From Liming Cai, who ran these:
" I only have CPU and peak memory for plastid barcodes with 200 Mb input data. These computational cost included organellar assembly, barcode extraction, and phylogeny. There are some outliers likely caused by failed assemblies (labeled in Note).
Liming" 

Let's read the data table.
```{r}
conventional_barcodes_df = read_delim("Malpighiaceae_pt_traditional_barcode_CPU_mem.tsv")
conventional_barcodes_df
```

Let's now plot the distribution of CPU time and peak memory across samples.

```{r}
conventional_barcodes_df = conventional_barcodes_df %>%
  mutate(peak_memory_gb=`Peak Memory (kbytes)`/1024/1024,
         Note = ifelse(is.na(Note),"None",Note)) 

conventional_barcodes_df 
```

```{r}
ggplot(conventional_barcodes_df, aes(x=`Total CPU (seconds)`,group = Note,fill=Note)) + 
  geom_histogram() +
  labs(title = "CPU time to process conventional barcodes",
       x = "Time (seconds)",
       y = " Number of accessions" ) +
  theme_minimal()

ggsave(filename="histo_time_conventional_barcodes.pdf",
       device = "pdf",
       width = 7,
       height = 5,
       units='in' )
```

```{r}
ggplot(conventional_barcodes_df, aes(x=peak_memory_gb,group = Note,fill=Note)) + 
  geom_histogram() +
  labs(title = "Peak memory usage process conventional barcodes",
       x = "Memory (GB)",
       y = " Number of accessions" ) +
  theme_minimal()

ggsave(filename="histo_memory_conventional_barcodes.pdf",
       device = "pdf",
       width = 7,
       height = 5,
       units='in' )
```
Averages
```{r}
conventional_barcodes_df %>% filter(Note=="None") %>%
  summarise(mean(`Total CPU (seconds)`),mean(peak_memory_gb))
```




## Skmer
Let's define a function to parse the skmer log:

```{r}
library(tidyverse)
library(stringr)

parse_skmer_log <- function(log_file_path) {
  # Read the entire file
  log_content <- readLines(log_file_path)
  
  # Helper function to convert time to seconds
  convert_to_seconds <- function(time_str) {
    if (grepl("^[0-9.]+$", time_str)) return(as.numeric(time_str))
    
    parts <- str_match(time_str, "([0-9]+)m([0-9.]+)s")[1,]
    if (length(parts) == 3) {
      mins <- as.numeric(parts[2])
      secs <- as.numeric(parts[3])
      return(mins * 60 + secs)
    }
    return(NA)
  }
  
  # Initialize results list
  results <- tibble(
    validation_sample = character(),
    phase = character(),
    data_amount_mbp = numeric(),
    User_time_seconds = numeric(),
    System_time_seconds = numeric(),
    Elapsed_time_seconds = numeric()
  )
  
  # Process file line by line
  i <- 1
  while (i <= length(log_content)) {
    line <- log_content[i]
    
    # Look for timing block followed by ELAPSED line
    if (grepl("^real\\s", line) && 
        i + 3 <= length(log_content) && 
        grepl("^ELAPSED (REFERENCE|QUERY)", log_content[i + 3])) {
      
      # Extract times
      real_time <- str_trim(sub("^real\\s+", "", line))
      user_time <- str_trim(sub("^user\\s+", "", log_content[i + 1]))
      sys_time <- str_trim(sub("^sys\\s+", "", log_content[i + 2]))
      
      # Parse ELAPSED line
      elapsed_line <- log_content[i + 3]
      phase <- str_match(elapsed_line, "ELAPSED (REFERENCE|QUERY)")[1,2]
      
      # Different parsing for REFERENCE and QUERY
      if (phase == "REFERENCE") {
        sample <- str_match(elapsed_line, "maxdata ([[:alnum:]]+):")[1,2]
        data_amount <- NA_real_
      } else {
        # For QUERY, parse both sample and data amount
        parsed <- str_match(elapsed_line, "maxdata ([[:alnum:]]+)_([0-9]+)K:")[1,]
        sample <- parsed[2]
        data_amount <- as.numeric(parsed[3]) / 1000  # Convert Kbp to Mbp
      }
      
      # Add to results if it's a REFERENCE or QUERY
      if (!is.na(phase) && phase %in% c("REFERENCE", "QUERY")) {
        results <- results %>%
          add_row(
            validation_sample = sample,
            phase = phase,
            data_amount_mbp = data_amount,
            User_time_seconds = convert_to_seconds(user_time),
            System_time_seconds = convert_to_seconds(sys_time),
            Elapsed_time_seconds = convert_to_seconds(real_time)
          )
      }
      
      i <- i + 4
    } else {
      i <- i + 1
    }
  }
  
  return(results)
}
```

Now let's apply this function

```{r}
df_skmer = parse_skmer_log("../varKoder_tests/Malpighiales/skmer/skmer.out")
df_skmer
```

Now let's get the average processing time
```{r}
training_summary_skmer = df_skmer %>%
  filter(phase=="REFERENCE") %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),)

training_summary_skmer
```
And the average query time. For compatibility with varKoder, we will sum the time for all query samples
```{r}
query_summary_skmer = df_skmer %>%
  filter(phase=="QUERY") %>%
  group_by(validation_sample) %>%
  select(-phase,-data_amount_mbp) %>%
  summarise_all(sum) %>%
  ungroup() %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),)

query_summary_skmer
```
Now without summing
```{r}
query_summary_skmer = df_skmer %>%
  filter(phase=="QUERY") %>%
  select(-phase,-data_amount_mbp) %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),)

query_summary_skmer
```

## NCBI SRA downloads

```{r}
df_SRA = read_csv("../varKoder_tests/all_SRA_taxa/computing_resources/sample_level_summary.csv") %>%
  filter(output_file == "5_varKodes_processed.out") %>%
  group_by(accession) %>%
  summarize(duration = sum(duration_seconds))

df_SRA

ggplot(df_SRA,aes(x=duration)) +
  geom_histogram(bins = 500) +
  scale_y_sqrt(breaks=c(1,5,10,50,100,500,1000,5000,10000,30000)) +
  scale_x_continuous(breaks = c(0,20,50,100,150,200,300,400,600,800)) +
  labs(title = "Time to download ~20Mb of SRA reads",
       x = "Time (seconds)",
       y = " Number of accessions" ) +
  theme_minimal()

ggsave(filename="histo_time_SRA_download.pdf",
       device = "pdf",
       width = 7,
       height = 5,
       units='in' )
```
What is the average time?
```{r}
df_SRA %>% ungroup %>% summarise(mean(duration))
```
Now let's evaluate time to process chunks. Let's ignore chunks that processed fewer than 450 samples
```{r}
df_SRA_chunks = read_csv("../varKoder_tests/all_SRA_taxa/computing_resources/varkoder_runs_summary.csv") %>%
  filter(output_file == "5_varKodes_processed.out",
         samples_started > 450) %>%
  select(chunk_id,duration_seconds,samples_started,samples_completed)

df_SRA_chunks
```

Let's calculate the overall completion rate:
```{r}
df_SRA_chunks %>%
  summarise(sum(samples_completed)/sum(samples_started))
```

Let's now plot a graph of the execution time against number of samples.
```{r}
ggplot(df_SRA_chunks) +
  geom_jitter(aes(x=samples_started,y=duration_seconds)) +
  labs(title = "Time to process each chunk with varkoder image",
       x = "Number of input accessions",
       y = "Execution time (s)" ) +
  theme_minimal()

ggsave(filename="time_SRA_processing.pdf",
       device = "pdf",
       width = 7,
       height = 5,
       units='in' )
```
On average, how many seconds per sample?
```{r}
df_SRA_chunks %>%
  summarise(sum(duration_seconds)/sum(samples_started))
```





