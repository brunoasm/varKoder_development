---
title: "Creating tables from profiling results"
output: html_notebook
---

```{r}
library(tidyverse)
```


In this document, we will summarize profiling results in different computers and different tasks to create a supplement on computing resources.

## Bembidion tests

Let's start with a test using Bembidion data only.Let's create a function to read a log file and convert to a table.
```{r}
parse_profiling_log <- function(log_file_path) {
  # Helper function to convert time string to seconds
  convert_to_seconds <- function(time_str) {
    if (is.null(time_str) || is.na(time_str)) return(NA)
    
    parts <- str_split(time_str, ":")[[1]]
    if (length(parts) == 2) {
      # m:ss format
      mins <- as.numeric(parts[1])
      secs <- as.numeric(sub("\\..*$", "", parts[2]))
      return(mins * 60 + secs)
    } else if (length(parts) == 3) {
      # h:mm:ss format
      hours <- as.numeric(parts[1])
      mins <- as.numeric(parts[2])
      secs <- as.numeric(sub("\\..*$", "", parts[3]))
      return(hours * 3600 + mins * 60 + secs)
    }
    return(NA)
  }
  
  # Read the entire file
  log_content <- readLines(log_file_path)
  
  # Initialize lists to store results
  commands <- list()
  metrics <- list()
  
  current_command <- NULL
  current_metrics <- list()
  
  # Process each line
  for (line in log_content) {
    # Look for command being timed
    if (grepl("Command being timed:", line)) {
      if (!is.null(current_command)) {
        commands <- append(commands, list(current_command))
        metrics <- append(metrics, list(current_metrics))
      }
      current_command <- str_trim(sub("Command being timed: \"", "", sub("\"$", "", line)))
      current_metrics <- list()
      next
    }
    
    # Parse metric lines
    if (grepl("^[A-Z0-9]+: User time \\(seconds\\): ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["User_time_seconds"]] <- as.numeric(parts[3])
    } else if (grepl("^[A-Z0-9]+: System time \\(seconds\\): ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["System_time_seconds"]] <- as.numeric(parts[3])
    } else if (grepl("^[A-Z0-9]+: Percent of CPU this job got: ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["Percent_of_CPU"]] <- as.numeric(sub("%", "", parts[3]))
    } else if (grepl("^[A-Z0-9]+: Maximum resident set size \\(kbytes\\): ", line)) {
      parts <- str_split(line, ": ", n = 3)[[1]]
      current_metrics[["Maximum_resident_set_size_kbytes"]] <- as.numeric(parts[3])
    } else if (grepl("^[A-Z0-9]+: Elapsed \\(wall clock\\) time", line)) {
      time_value <- str_extract(line, "[0-9]+:[0-9]+(?:\\.[0-9]+)?$|[0-9]+:[0-9]+:[0-9]+(?:\\.[0-9]+)?$")
      current_metrics[["Elapsed_time_seconds"]] <- convert_to_seconds(time_value)
    }
  }
  
  # Add the last command if exists
  if (!is.null(current_command)) {
    commands <- append(commands, list(current_command))
    metrics <- append(metrics, list(current_metrics))
  }
  
  # Convert to tibble
  result_df <- tibble(
    code = str_extract(unlist(commands), "^\\S+"),
    User_time_seconds = sapply(metrics, `[[`, "User_time_seconds"),
    System_time_seconds = sapply(metrics, `[[`, "System_time_seconds"),
    Elapsed_time_seconds = sapply(metrics, `[[`, "Elapsed_time_seconds"),
    Percent_of_CPU = sapply(metrics, `[[`, "Percent_of_CPU"),
    maxRSS_GB = sapply(metrics, `[[`, "Maximum_resident_set_size_kbytes") / (1024 * 1024),
    command = str_trim(str_remove(unlist(commands), "^\\S+ "))
  )
  
  return(result_df)
}
```

Let's now apply this function.
```{r}
list.files(pattern=".*tests.log") %>%
    set_names(., tools::file_path_sans_ext(.)) %>%
    purrr::map_df(parse_profiling_log, .id = "filename")

```

## CGR vs representation test

Let's start by reading the data table and standardizing column names to the table above. This table was generated by a python script parsing a log file in the analysis folder.
```{r}
df = read_csv("../varKoder_tests/additional_tests_cgr/computing_summary.csv") %>%
  mutate(maxRSS_GB=max_resident_kb/(1024*1024)) %>%
  select(phase, 
         architecture,
         representation,
         validation_sample,
         Elapsed_time_seconds=elapsed_time,
         User_time_seconds=user_time,
         System_time_seconds=system_time,
         Percent_of_CPU=cpu_percent,
         maxRSS_GB
         )

df
```

Let's now ask:

What is the average time taken for training 99 samples for each combination of architecture. We have to sum the time of both training phases.
```{r}
training_summary = df %>%
  filter(phase!='query') %>%
  group_by(architecture,representation,validation_sample) %>%
  summarise(User_time_seconds=sum(User_time_seconds),
            Elapsed_time_seconds=sum(Elapsed_time_seconds),
            System_time_seconds=sum(System_time_seconds),
            Percent_of_CPU=mean(Percent_of_CPU),
            maxRSS_GB=max(maxRSS_GB)) %>%
  group_by(architecture,representation) %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),
            Mean_Percent_of_CPU=mean(Percent_of_CPU),
            mean_maxRSS_GB=mean(maxRSS_GB))

training_summary
```
What is the average time taken for for querying all varKodes for each sample?
```{r}
query_summary = df %>%
  filter(phase=='query') %>%
  group_by(architecture,representation,validation_sample) %>%
  summarise(User_time_seconds=sum(User_time_seconds),
            Elapsed_time_seconds=sum(Elapsed_time_seconds),
            System_time_seconds=sum(System_time_seconds),
            Percent_of_CPU=mean(Percent_of_CPU),
            maxRSS_GB=max(maxRSS_GB)) %>%
  group_by(architecture,representation) %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),
            Mean_Percent_of_CPU=mean(Percent_of_CPU),
            mean_maxRSS_GB=mean(maxRSS_GB))

query_summary
```

## Skmer
Let's define a function to parse the skmer log:

```{r}
library(tidyverse)
library(stringr)

parse_skmer_log <- function(log_file_path) {
  # Read the entire file
  log_content <- readLines(log_file_path)
  
  # Helper function to convert time to seconds
  convert_to_seconds <- function(time_str) {
    if (grepl("^[0-9.]+$", time_str)) return(as.numeric(time_str))
    
    parts <- str_match(time_str, "([0-9]+)m([0-9.]+)s")[1,]
    if (length(parts) == 3) {
      mins <- as.numeric(parts[2])
      secs <- as.numeric(parts[3])
      return(mins * 60 + secs)
    }
    return(NA)
  }
  
  # Initialize results list
  results <- tibble(
    validation_sample = character(),
    phase = character(),
    data_amount_mbp = numeric(),
    User_time_seconds = numeric(),
    System_time_seconds = numeric(),
    Elapsed_time_seconds = numeric()
  )
  
  # Process file line by line
  i <- 1
  while (i <= length(log_content)) {
    line <- log_content[i]
    
    # Look for timing block followed by ELAPSED line
    if (grepl("^real\\s", line) && 
        i + 3 <= length(log_content) && 
        grepl("^ELAPSED (REFERENCE|QUERY)", log_content[i + 3])) {
      
      # Extract times
      real_time <- str_trim(sub("^real\\s+", "", line))
      user_time <- str_trim(sub("^user\\s+", "", log_content[i + 1]))
      sys_time <- str_trim(sub("^sys\\s+", "", log_content[i + 2]))
      
      # Parse ELAPSED line
      elapsed_line <- log_content[i + 3]
      phase <- str_match(elapsed_line, "ELAPSED (REFERENCE|QUERY)")[1,2]
      
      # Different parsing for REFERENCE and QUERY
      if (phase == "REFERENCE") {
        sample <- str_match(elapsed_line, "maxdata ([[:alnum:]]+):")[1,2]
        data_amount <- NA_real_
      } else {
        # For QUERY, parse both sample and data amount
        parsed <- str_match(elapsed_line, "maxdata ([[:alnum:]]+)_([0-9]+)K:")[1,]
        sample <- parsed[2]
        data_amount <- as.numeric(parsed[3]) / 1000  # Convert Kbp to Mbp
      }
      
      # Add to results if it's a REFERENCE or QUERY
      if (!is.na(phase) && phase %in% c("REFERENCE", "QUERY")) {
        results <- results %>%
          add_row(
            validation_sample = sample,
            phase = phase,
            data_amount_mbp = data_amount,
            User_time_seconds = convert_to_seconds(user_time),
            System_time_seconds = convert_to_seconds(sys_time),
            Elapsed_time_seconds = convert_to_seconds(real_time)
          )
      }
      
      i <- i + 4
    } else {
      i <- i + 1
    }
  }
  
  return(results)
}
```

Now let's apply this function

```{r}
df_skmer = parse_skmer_log("../varKoder_tests/Malpighiales/skmer/skmer.out")
df_skmer
```

Now let's get the average processing time
```{r}
training_summary_skmer = df_skmer %>%
  filter(phase=="REFERENCE") %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),)

training_summary_skmer
```
And the average query time. For compatibility with varKoder, we will sum the time for all query samples
```{r}
query_summary_skmer = df_skmer %>%
  filter(phase=="QUERY") %>%
  group_by(validation_sample) %>%
  select(-phase,-data_amount_mbp) %>%
  summarise_all(sum) %>%
  ungroup() %>%
  summarise(Mean_User_time_seconds=mean(User_time_seconds),
            Mean_Elapsed_time_seconds=mean(Elapsed_time_seconds),
            Mean_System_time_seconds=mean(System_time_seconds),)

query_summary_skmer
```



